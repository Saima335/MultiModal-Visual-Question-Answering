# Multimodal Reasoning Visual Question Answering (VQA)

This project focuses on multimodal reasoning for Visual Question Answering (VQA), leveraging image captioning and large language models for reasoning.

## ğŸ§  Overview

The system performs the following:
1. Takes an image and a question.
2. Generates a caption using an image captioning model (Gemini API).
3. Performs reasoning using the caption and question via DeepSeek-R1-distill-llama-70b (Grok API).
4. Outputs the answer in a `pred.json` file.

## ğŸ“ Project Structure

```

project/
â”‚
â”œâ”€â”€ testing/
â”‚ â”œâ”€â”€ mistral.py # llava-v1.6-mistral-7b-hf experiments (not used)
â”‚ â”œâ”€â”€ deepseek.py # other deepseek experiments
â”‚ â”œâ”€â”€ ocr.py # OCR-based models
â”‚ â””â”€â”€ image_captioning.py # Other image captioning models (blip)
|
â”œâ”€â”€ test_images/ # contain test images
â”‚
â”œâ”€â”€ evaluate.py # Generates pred.json with final answers
â”œâ”€â”€ evaluate_results.py # Compares pred.json and gold.json, computes accuracy
â”œâ”€â”€ pred.json # Predictions generated
â”œâ”€â”€ gold.json # Ground truth labels
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

## ğŸ§ª Evaluation

Run the following commands to generate predictions and evaluate:

```bash
python evaluate.py
python evaluate_results.py
````

Output:

* `pred.json`: Contains list of dicts with `id`, `language`, `answer_key`
* Accuracy is printed after comparing with `gold.json`.

## ğŸ”§ APIs Used

* **Google Gemini**: For image captioning
* **DeepSeek LLaMA-70B (Distilled)**: For reasoning

  > API keys are managed through Google Console (Gemini) and Grok (DeepSeek)

## âš™ï¸ Requirements

Install dependencies using:

```bash
pip install -r requirements.txt
```

## ğŸ“Œ Notes

* Initial experiments with `llava-v1.6-mistral`, `deepseek`, OCR, and local captioning gave poor results.
* Final pipeline uses Gemini + DeepSeek APIs.

````
